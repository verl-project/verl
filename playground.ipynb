{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visible cuda devices: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"visible cuda devices:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrated from from verl.utils.reward_score.math_verify import compute_score\n",
    "# Reduce reliance on installing verl, evaluation should be verl free..\n",
    "try:\n",
    "    from math_verify.errors import TimeoutException\n",
    "    from math_verify.metric import math_metric\n",
    "    from math_verify.parser import ExprExtractionConfig, LatexExtractionConfig\n",
    "except ImportError:\n",
    "    print(\"To use Math-Verify, please install it first by running `pip install math-verify`.\")\n",
    "\n",
    "\n",
    "def compute_score(model_output: str, ground_truth: str, timeout_score: float = 0) -> bool:\n",
    "    verify_func = math_metric(\n",
    "        gold_extraction_target=(LatexExtractionConfig(),),\n",
    "        pred_extraction_target=(ExprExtractionConfig(), LatexExtractionConfig()),\n",
    "    )\n",
    "    ret_score = 0.0\n",
    "\n",
    "    # Wrap the ground truth in \\boxed{} format for verification\n",
    "    ground_truth_boxed = \"\\\\boxed{\" + ground_truth + \"}\"\n",
    "    try:\n",
    "        ret_score, _ = verify_func([ground_truth_boxed], [model_output])\n",
    "    except Exception:\n",
    "        pass\n",
    "    except TimeoutException:\n",
    "        ret_score = timeout_score\n",
    "\n",
    "    return ret_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traverse Ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class CheckpointRef:\n",
    "    exp_dir: Path\n",
    "    exp_name: str\n",
    "    step: int\n",
    "    step_dir: Path\n",
    "    actor_dir: Path\n",
    "\n",
    "def parse_step_from_stepdir(step_dir_name: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Support:\n",
    "      global_step_60\n",
    "      global_steps_60\n",
    "      step_60\n",
    "      global_step60 (less common)\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(global_steps|global_step|step)[^\\d]*(\\d+)\", step_dir_name)\n",
    "    return int(m.group(2)) if m else None\n",
    "\n",
    "def find_actor_dir(step_dir: Path) -> Path:\n",
    "    direct = step_dir / \"actor\"\n",
    "    if direct.is_dir():\n",
    "        return direct\n",
    "    cands = [p for p in step_dir.rglob(\"actor\") if p.is_dir()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"Cannot find actor dir under {step_dir}\")\n",
    "    # Prefer the one that looks like FSDP dir\n",
    "    for p in cands:\n",
    "        if (p / \"fsdp_config.json\").exists():\n",
    "            return p\n",
    "    return cands[0]\n",
    "\n",
    "def scan_exp_dir(exp_dir: str) -> List[CheckpointRef]:\n",
    "    \"\"\"\n",
    "    Scan exactly one experiment directory:\n",
    "      <exp_dir>/\n",
    "        global_step_0/\n",
    "          actor/...\n",
    "        global_step_60/\n",
    "          actor/...\n",
    "        ...\n",
    "\n",
    "    Returns sorted list of CheckpointRef by step.\n",
    "    \"\"\"\n",
    "    exp = Path(exp_dir).expanduser().resolve()\n",
    "    if not exp.is_dir():\n",
    "        raise FileNotFoundError(f\"exp_dir not found: {exp}\")\n",
    "\n",
    "    out: List[CheckpointRef] = []\n",
    "    for child in exp.iterdir():\n",
    "        if not child.is_dir():\n",
    "            continue\n",
    "        step = parse_step_from_stepdir(child.name)\n",
    "        if step is None:\n",
    "            continue\n",
    "        actor_dir = find_actor_dir(child)\n",
    "        out.append(CheckpointRef(\n",
    "            exp_dir=exp,\n",
    "            exp_name=exp.name,\n",
    "            step=step,\n",
    "            step_dir=child,\n",
    "            actor_dir=actor_dir,\n",
    "        ))\n",
    "\n",
    "    out.sort(key=lambda x: x.step)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CheckpointRef(exp_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2'), exp_name='openthoughts-grpo-qwen3_0p6b_fsdp2', step=60, step_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_60'), actor_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_60/actor')),\n",
       " CheckpointRef(exp_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2'), exp_name='openthoughts-grpo-qwen3_0p6b_fsdp2', step=120, step_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_120'), actor_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_120/actor')),\n",
       " CheckpointRef(exp_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2'), exp_name='openthoughts-grpo-qwen3_0p6b_fsdp2', step=180, step_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_180'), actor_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_180/actor')),\n",
       " CheckpointRef(exp_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2'), exp_name='openthoughts-grpo-qwen3_0p6b_fsdp2', step=240, step_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_240'), actor_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_240/actor')),\n",
       " CheckpointRef(exp_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2'), exp_name='openthoughts-grpo-qwen3_0p6b_fsdp2', step=300, step_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_300'), actor_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_300/actor')),\n",
       " CheckpointRef(exp_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2'), exp_name='openthoughts-grpo-qwen3_0p6b_fsdp2', step=319, step_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_319'), actor_dir=PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/global_step_319/actor'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint_dir = \"/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2\"\n",
    "\n",
    "ckpts = scan_exp_dir(checkpoint_dir)\n",
    "ckpts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_merged_hf_dir(model_dir: Path) -> bool:\n",
    "    return model_dir.is_dir() and (model_dir / \"config.json\").exists()\n",
    "\n",
    "def default_cache_root_for_exp(exp_dir: Path) -> Path:\n",
    "    # <exp_dir>/_merged_hf_cache/global_step_<step>\n",
    "    return exp_dir / \"_merged_hf_cache\"\n",
    "\n",
    "def default_target_dir_for_ckpt(ckpt: CheckpointRef) -> Path:\n",
    "    return default_cache_root_for_exp(ckpt.exp_dir) / f\"global_step_{ckpt.step}\"\n",
    "\n",
    "def run_cmd_capture(cmd):\n",
    "    print(\"[CMD]\", \" \".join(map(str, cmd)))\n",
    "    p = subprocess.run(list(map(str, cmd)), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError(f\"rc={p.returncode}\\nSTDOUT:\\n{p.stdout[-4000:]}\\nSTDERR:\\n{p.stderr[-4000:]}\\n\")\n",
    "    return p.stdout, p.stderr\n",
    "\n",
    "def merge_one_fsdp_ckpt(\n",
    "    ckpt: CheckpointRef,\n",
    "    backend: str = \"fsdp\",\n",
    "    force: bool = False,\n",
    "    python_exec: Optional[str] = None,\n",
    ") -> Path:\n",
    "    tgt = default_target_dir_for_ckpt(ckpt)\n",
    "    if is_merged_hf_dir(tgt) and not force:\n",
    "        print(f\"[skip] {tgt}\")\n",
    "        return tgt\n",
    "    if force and tgt.exists():\n",
    "        shutil.rmtree(tgt)\n",
    "    tgt.mkdir(parents=True, exist_ok=True)\n",
    "    if python_exec is None:\n",
    "        python_exec = sys.executable\n",
    "    cmd = [\n",
    "        python_exec, \"-m\", \"verl.model_merger\", \"merge\",\n",
    "        \"--backend\", backend,\n",
    "        \"--local_dir\", str(ckpt.actor_dir),\n",
    "        \"--target_dir\", str(tgt),\n",
    "    ]\n",
    "    run_cmd_capture(cmd)\n",
    "    print(f\"[ok] merged -> {tgt}\")\n",
    "    return tgt\n",
    "\n",
    "def merge_all_steps_for_exp_dir(\n",
    "    exp_dir: str,\n",
    "    backend: str = \"fsdp\",\n",
    "    force: bool = False,\n",
    ") -> List[Tuple[int, Path]]:\n",
    "    ckpts = scan_exp_dir(exp_dir)\n",
    "    out = []\n",
    "    for c in ckpts:\n",
    "        tgt = merge_one_fsdp_ckpt(c, backend=backend, force=force)\n",
    "        out.append((c.step, tgt))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60\n",
      "[skip] /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_120\n",
      "[skip] /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_180\n",
      "[skip] /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_240\n",
      "[skip] /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_300\n",
      "[skip] /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(60,\n",
       "   PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60')),\n",
       "  (120,\n",
       "   PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_120')),\n",
       "  (180,\n",
       "   PosixPath('/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_180'))],\n",
       " 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = merge_all_steps_for_exp_dir(checkpoint_dir, backend=\"fsdp\", force=False)\n",
    "merged[:3], len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 60 /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to a large language model. Let me start by recalling the key points. Large language models are AI systems designed to understand and generate human language, right? They can process text in multiple languages and perform various tasks like answering questions, writing, or even creative writing.\n",
      "\n",
      "I should mention that they're trained on massive amounts of text, which allows them to learn complex patterns and nuances. Also, they can handle multiple languages and adapt to different contexts. Maybe include how they're used in various applications like customer service, content creation, or research.\n",
      "\n",
      "Wait, should I add something about the training data and the model's capabilities? That could make it more comprehensive. Also, maybe touch on their efficiency and performance. Make sure the language is simple and engaging for someone who might not be familiar with the technical terms. Avoid jargon but still sound informative. Let me check if I'm covering all essential aspects without being too lengthy.\n",
      "</think>\n",
      "content: A large language model is an AI system designed to understand and generate human language. It can process text in multiple languages and perform a wide range of tasks, such as answering questions, writing content, or even creative writing. These models are trained on massive datasets to learn complex patterns and nuances in language, allowing them to adapt to different contexts and provide accurate responses.\n"
     ]
    }
   ],
   "source": [
    "step, model_dir = merged[0]   \n",
    "model_dir = Path(model_dir)\n",
    "print(\"Testing:\", step, model_dir)\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "model_name = model_dir\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _de_numpy(x):\n",
    "    \"\"\"Convert numpy containers/scalars into pure Python recursively.\"\"\"\n",
    "    # numpy scalar -> python scalar\n",
    "    if isinstance(x, np.generic):\n",
    "        return x.item()\n",
    "    # numpy array -> list (then recurse)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return [_de_numpy(v) for v in x.tolist()]\n",
    "    # dict\n",
    "    if isinstance(x, dict):\n",
    "        return {k: _de_numpy(v) for k, v in x.items()}\n",
    "    # list/tuple\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [_de_numpy(v) for v in x]\n",
    "    return x\n",
    "\n",
    "def load_records_from_parquet(parquet_path: str):\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    records = df.to_dict(\"records\")\n",
    "\n",
    "    # Fix prompt + sanitize numpy types\n",
    "    fixed = []\n",
    "    for r in records:\n",
    "        r = _de_numpy(r)\n",
    "\n",
    "        # Ensure prompt is a python list of message dicts\n",
    "        # Your data sometimes has prompt as np.ndarray([{'role':..., 'content':...}], dtype=object)\n",
    "        p = r.get(\"prompt\", None)\n",
    "        if p is None:\n",
    "            r[\"prompt\"] = []\n",
    "        elif isinstance(p, dict):\n",
    "            # (edge) single message dict -> list\n",
    "            r[\"prompt\"] = [p]\n",
    "        elif isinstance(p, list):\n",
    "            r[\"prompt\"] = p\n",
    "        else:\n",
    "            # if some weird type survived, try best-effort\n",
    "            r[\"prompt\"] = list(p)\n",
    "\n",
    "        fixed.append(r)\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"/mnt/local/shared/michaelw/mlf2/verl/reproduce/data/openthoughts3/local_parquet_dir/test.parquet\"\n",
    "records = load_records_from_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4787"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_source': 'open-thoughts/OpenThoughts3-1.2M',\n",
       " 'prompt': [{'content': \"Let's think step by step and solve this problem. Find the smallest possible value of the sum $\\\\lvert x + 1\\\\rvert + \\\\lvert x + 3\\\\rvert + \\\\lvert x + 7\\\\rvert$.\",\n",
       "   'role': 'user'}],\n",
       " 'ability': 'math',\n",
       " 'reward_model': {'ground_truth': '6', 'style': 'rule'},\n",
       " 'extra_info': {'answer': '<think> Okay, so I need to find the smallest possible value of the sum |x + 1| + |x + 3| + |x + 7|. Hmm, absolute value expressions can sometimes be tricky because they change their behavior depending on whether the inside is positive or negative. I remember that the sum of absolute values often has its minimum at a median point or something like that. Maybe I should think about how the different terms behave as x changes.\\n\\nFirst, let me recall that the absolute value function |a| is equal to a when a ≥ 0 and -a when a < 0. So, each term in the sum |x +1|, |x +3|, and |x +7| will change their expressions when x is equal to -1, -3, or -7 respectively. These points, -7, -3, and -1 are the critical points where the expressions inside the absolute values switch from negative to positive. Therefore, the behavior of the sum will change at these points. To find the minimum, maybe I should check the value of the sum in each interval defined by these critical points and see where the minimum occurs.\\n\\nThe critical points divide the real number line into four intervals:\\n\\n1. x < -7\\n2. -7 ≤ x < -3\\n3. -3 ≤ x < -1\\n4. x ≥ -1\\n\\nI need to analyze the expression in each interval and see how the sum behaves. Let me consider each interval one by one.\\n\\nStarting with the first interval, x < -7:\\n\\nIn this case, since x is less than -7, all the expressions inside the absolute values will be negative. \\n\\nSo, |x +1| = -(x + 1) = -x -1\\n\\nSimilarly, |x + 3| = -x -3\\n\\nand |x +7| = -x -7\\n\\nTherefore, the sum becomes:\\n\\n(-x -1) + (-x -3) + (-x -7) = -3x -11\\n\\nNow, since x < -7, the coefficient of x here is -3, which is negative. That means as x increases towards -7, the sum decreases. So in this interval, the minimum would occur as close to x = -7 as possible, but since we can\\'t include x = -7 here (since this is the interval x < -7), but approaching -7 from the left, the sum approaches -3*(-7) -11 = 21 -11 = 10.\\n\\nWait, actually, maybe I should think about the behavior of the function in each interval. Since in the interval x < -7, the sum is a linear function with slope -3. Since slope is negative, the function is decreasing as x increases. Therefore, the minimal value in this interval as x approaches -7 from the left will give the minimal value here, which is 10. But in the next interval, from -7 to -3, the slope might be different. Let\\'s check that.\\n\\nMoving on to the interval -7 ≤ x < -3:\\n\\nIn this interval, x is greater than or equal to -7, so |x +7| becomes x +7, since x +7 ≥ 0 here. The other terms, x +1 and x +3, are still negative because x is less than -3, so -3 is the upper limit here. \\n\\nSo:\\n\\n|x +1| = -x -1\\n\\n|x +3| = -x -3\\n\\n|x +7| = x +7\\n\\nAdding them up: (-x -1) + (-x -3) + (x +7) = (-x -x + x) + (-1 -3 +7) = (-x) + 3\\n\\nSo the sum becomes -x + 3 in this interval. Now, the slope here is -1. Since the coefficient of x is -1, the function is decreasing again as x increases. So again, in this interval, the sum decreases as x increases, so the minimal value in this interval would occur at the right end, approaching x = -3 from the left. Let me compute the value at x = -3 (even though it\\'s not included in this interval, we can check the limit):\\n\\nPlugging x approaching -3 from left into the expression -x + 3 gives -(-3) + 3 = 3 + 3 = 6. However, at x = -3, we enter the next interval, so let\\'s look there.\\n\\nThird interval: -3 ≤ x < -1:\\n\\nHere, x is between -3 and -1. So, x + 3 is non-negative, and x +1 is still negative (since x is less than -1). x +7 is also positive since -3 is greater than -7, so x+7 will be at least (-3)+7 = 4, which is positive. Thus, the absolute values can be written as:\\n\\n|x +1| = -x -1\\n\\n|x +3| = x +3\\n\\n|x +7| = x +7\\n\\nAdding them up: (-x -1) + (x +3) + (x +7) = (-x + x + x) + (-1 +3 +7) = (x) + 9\\n\\nTherefore, the sum becomes x + 9 in this interval. Now the slope here is 1, which is positive. So this function is increasing as x increases. Therefore, the minimal value in this interval is at the left end, which is at x = -3. Let\\'s compute that.\\n\\nAt x = -3, substituting into x + 9: -3 + 9 = 6. Which matches the limit from the previous interval. Since the function is increasing here, moving to the right increases the sum, so the minimal in this interval is indeed 6.\\n\\nFourth interval: x ≥ -1:\\n\\nNow, in this interval, all the expressions inside the absolute values are non-negative.\\n\\n|x +1| = x +1\\n\\n|x +3| = x +3\\n\\n|x +7| = x +7\\n\\nSo sum is (x +1) + (x +3) + (x +7) = 3x + 11.\\n\\nHere, the slope is 3, positive, so the function is increasing as x increases. So the minimal value in this interval is at the left end, which is x = -1. Let\\'s compute that.\\n\\nAt x = -1, the sum is 3*(-1) + 11 = -3 +11 = 8.\\n\\nSo summarizing the behavior in each interval:\\n\\n1. For x < -7: The sum is -3x -11, decreasing, approaching 10 at x = -7.\\n2. For -7 ≤ x < -3: The sum is -x +3, decreasing, approaching 6 at x = -3.\\n3. For -3 ≤ x < -1: The sum is x + 9, increasing, starting at 6 when x = -3 and going up to 8 at x = -1.\\n4. For x ≥ -1: The sum is 3x +11, increasing, starting at 8 and going up as x increases.\\n\\nTherefore, putting all this together, the function decreases until x = -3, reaching a minimum of 6, then starts increasing. Therefore, the minimal value is 6, achieved at x = -3. Wait, but hold on a second. Let me confirm.\\n\\nWait, in the interval -3 ≤ x < -1, the expression is x +9. So at x = -3, that gives -3 +9 = 6. At x = -1, it\\'s -1 +9 = 8. But in the next interval, starting at x = -1, it starts at 8, so indeed the minimal value seems to be at x = -3. However, in the third interval, the function is increasing from x = -3 onwards, so the minimum there is at x = -3.\\n\\nWait, but what about between -7 and -3? In that interval, the function is -x +3, which is decreasing as x approaches -3 from the left. So approaching x = -3 from the left, the value approaches 6. Then at x = -3, it continues with the same value, 6, and starts increasing. Therefore, the function is smooth at x = -3? Wait, let\\'s check if there\\'s a corner there.\\n\\nLet me calculate the left and right derivatives at x = -3. \\n\\nIn the interval to the left of -3 (i.e., -7 to -3), the slope is -1. In the interval to the right (from -3 onwards, up to -1), the slope is +1. Therefore, the function\\'s slope goes from -1 to +1 at x = -3, which means there\\'s a corner there, and x = -3 is indeed the point where the function stops decreasing and starts increasing, so the minimum is at x = -3.\\n\\nWait, so then the minimal value is 6? But let me confirm by plugging x = -3 into the original expression:\\n\\n| -3 +1 | + | -3 +3 | + | -3 +7 | = | -2 | + | 0 | + | 4 | = 2 + 0 + 4 = 6. Yup, that\\'s correct.\\n\\nSimilarly, if I check at x = -7, just to see, what would the value be?\\n\\n|x +1| + |x +3| + |x +7| at x = -7:\\n\\n| -7 +1 | + | -7 +3 | + | -7 +7 | = | -6 | + | -4 | + 0 = 6 + 4 + 0 = 10. Which is the same as the limit from the left interval\\'s approach.\\n\\nSimilarly, at x = -1, the value is:\\n\\n| -1 +1 | + | -1 +3 | + | -1 +7 | = 0 + 2 + 6 = 8, which matches.\\n\\nTherefore, seems like the minimal value is indeed 6 at x = -3.\\n\\nWait, but hold on, the problem says \"the smallest possible value of the sum\"—so is 6 the minimal?\\n\\nAlternatively, sometimes the minimum can occur over an interval if the slope is zero in that region.\\n\\nBut in this case, the slope is only changing from negative to positive at x = -3, so the minimum is achieved exactly at that point. \\n\\nAlternatively, if there is a point where slope transitions through zero, but in this case, the slopes are -3, -1, +1, +3. So at x = -3, slope goes from -1 to +1. Therefore, there is no interval where the slope is zero. The minimal value is achieved precisely at x = -3. Therefore, 6 is indeed the minimal value.\\n\\nAlternatively, is there another approach to solve this problem?\\n\\nI remember that for the sum of absolute deviations, the minimum is achieved at the median of the points.\\n\\nLet me recall that concept. Suppose you have points on the real line, and you want to find the point x that minimizes the sum of absolute deviations from x to each of the points. The solution is the median of the points. Hmm, so in this problem, perhaps the expression is similar?\\n\\nWait, let\\'s see: The given expression is |x +1| + |x +3| + |x +7|, which can be rewritten as |x - (-1)| + |x - (-3)| + |x - (-7)|. So this is the sum of distances from x to the points -1, -3, and -7 on the real line. Therefore, indeed, we are to find the x that minimizes the sum of distances to -7, -3, and -1. So since there are three points, the median is the middle one, which is -3. Therefore, the minimal sum is achieved when x is the median, i.e., at x = -3, and the sum is the sum of distances from -3 to each of these points.\\n\\nCalculating that:\\n\\nDistance from -3 to -7 is 4,\\n\\nFrom -3 to -3 is 0,\\n\\nFrom -3 to -1 is 2.\\n\\nTotal sum is 4 + 0 + 2 = 6. Which matches the earlier result. So that method gives the same answer, so that confirms it.\\n\\nTherefore, regardless of the approach, the minimal sum is 6, achieved at x = -3.\\n\\nWait, but to be thorough, let me imagine another way. Let me think of it as a function f(x) = |x +1| + |x +3| + |x +7|. The derivative can be considered in each interval.\\n\\nIn regions where the function is smooth (i.e., not at the critical points), the derivative is the sum of the derivatives of each absolute value term.\\n\\nRecall that the derivative of |x - a| is -1 if x < a, and +1 if x > a. At x = a, the derivative is not defined (there\\'s a corner). So for our function f(x):\\n\\nThe derivative f’(x) is:\\n\\nIn regions where x < -7:\\n\\nEach term has derivative -1, so total derivative is -1 + -1 + -1 = -3.\\n\\nBetween -7 and -3:\\n\\nThe derivative of |x +7| becomes +1, while the other two are still -1 each, so total derivative is (-1) + (-1) + (1) = -1.\\n\\nBetween -3 and -1:\\n\\nThe derivative of |x +3| becomes +1, so derivatives for each term: |x +1| still -1, |x +3| is +1, |x +7| is +1. Total derivative: (-1) +1 +1 = +1.\\n\\nFor x > -1:\\n\\nAll derivatives are +1, so total derivative is +3.\\n\\nTherefore, the derivative goes from -3 to -1 to +1 to +3 as x crosses the critical points. Hence, the function is decreasing until x = -3, then starts increasing. Therefore, the minimum at x = -3 is confirmed.\\n\\nAlternatively, maybe another way, if I graph the function, the sum of absolute values. Each absolute value term is a V-shaped graph, and the sum will be a piecewise linear function with possible minima where the slope changes from negative to positive. Since in three dimensions it\\'s hard, but in lines, just plotting the slopes as I did above, it\\'s clear.\\n\\nAlternatively, perhaps trying specific points. Let me test x = -3, x = -4, x = -2, x = -5, etc., to see the value.\\n\\nAt x = -3: sum is 6 as we saw.\\n\\nAt x = -4:\\n\\n| -4 +1 | + | -4 +3 | + | -4 +7 | = | -3 | + | -1 | + | 3 | = 3 +1 +3=7, which is higher than 6.\\n\\nAt x = -2:\\n\\n| -2 +1| + | -2 +3 | + | -2 +7 | = 1 +1 +5=7, which is again higher.\\n\\nAt x = -5:\\n\\n| -5 +1 | + | -5 +3 | + | -5 +7 | = 4 + 2 +2 = 8, which is worse.\\n\\nx = -7:\\n\\nAs before, 10.\\n\\nx = -6:\\n\\n| -6 +1 | + | -6 +3 | + | -6 +7 | = 5 + 3 +1 =9.\\n\\nAll the other tested points give higher values than 6. What about between -3 and -1?\\n\\nLike x = -3.5? Wait, no, -3.5 is less than -3. Wait, between -3 and -1, let\\'s pick x = -2.5:\\n\\n| -2.5 +1 | + | -2.5 +3 | + | -2.5 +7 | = 1.5 + 0.5 +4.5 =6.5, which is higher than 6. So indeed, even in that interval, it\\'s above 6.\\n\\nTherefore, it seems 6 is the minimal value. \\n\\nTherefore, after all these checks, I believe the minimal sum is 6.\\n\\n**Final Answer**\\nThe smallest possible value of the sum is \\\\boxed{6}.\\n</think>\\n\\nTo find the smallest possible value of the sum \\\\( \\\\lvert x + 1 \\\\rvert + \\\\lvert x + 3 \\\\rvert + \\\\lvert x + 7 \\\\rvert \\\\), we analyze the behavior of the function in different intervals defined by the critical points \\\\(-7\\\\), \\\\(-3\\\\), and \\\\(-1\\\\).\\n\\n1. **Interval \\\\( x < -7 \\\\)**:\\n   - All terms are negative.\\n   - The sum is \\\\( -3x - 11 \\\\), which is a linear function with a slope of \\\\(-3\\\\), decreasing as \\\\( x \\\\) increases.\\n   - Approaching \\\\(-7\\\\) from the left, the value approaches \\\\( 10 \\\\).\\n\\n2. **Interval \\\\( -7 \\\\leq x < -3 \\\\)**:\\n   - \\\\( |x + 7| \\\\) is non-negative, others are negative.\\n   - The sum is \\\\( -x + 3 \\\\), which is a linear function with a slope of \\\\(-1\\\\), decreasing as \\\\( x \\\\) increases.\\n   - Approaching \\\\(-3\\\\) from the left, the value approaches \\\\( 6 \\\\).\\n\\n3. **Interval \\\\( -3 \\\\leq x < -1 \\\\)**:\\n   - \\\\( |x + 3| \\\\) and \\\\( |x + 7| \\\\) are non-negative, \\\\( |x + 1| \\\\) is negative.\\n   - The sum is \\\\( x + 9 \\\\), which is a linear function with a slope of \\\\(1\\\\), increasing as \\\\( x \\\\) increases.\\n   - At \\\\( x = -3 \\\\), the value is \\\\( 6 \\\\).\\n\\n4. **Interval \\\\( x \\\\geq -1 \\\\)**:\\n   - All terms are non-negative.\\n   - The sum is \\\\( 3x + 11 \\\\), which is a linear function with a slope of \\\\(3\\\\), increasing as \\\\( x \\\\) increases.\\n   - At \\\\( x = -1 \\\\), the value is \\\\( 8 \\\\).\\n\\nThe function decreases until \\\\( x = -3 \\\\) and then starts increasing. Therefore, the minimum value occurs at \\\\( x = -3 \\\\), giving the sum:\\n\\n\\\\[\\n\\\\lvert -3 + 1 \\\\rvert + \\\\lvert -3 + 3 \\\\rvert + \\\\lvert -3 + 7 \\\\rvert = 2 + 0 + 4 = 6\\n\\\\]\\n\\nThus, the smallest possible value of the sum is \\\\(\\\\boxed{6}\\\\).',\n",
       "  'answer_tokens': 527,\n",
       "  'cot_tokens': 3515,\n",
       "  'index': 95,\n",
       "  'is_answer_int': True,\n",
       "  'question': 'Find the smallest possible value of the sum $\\\\lvert x + 1\\\\rvert + \\\\lvert x + 3\\\\rvert + \\\\lvert x + 7\\\\rvert$.',\n",
       "  'split': 'validation'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Let's think step by step and solve this problem. Find the smallest possible value of the sum $\\\\lvert x + 1\\\\rvert + \\\\lvert x + 3\\\\rvert + \\\\lvert x + 7\\\\rvert$.\",\n",
       " 'role': 'user'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0][\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0][\"reward_model\"][\"ground_truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vllm_llm(\n",
    "    model_dir: str,\n",
    "    gpu_memory_utilization: float = 0.8,\n",
    "    dtype: str = \"auto\",\n",
    "    max_model_len: int | None = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    ") -> LLM:\n",
    "    kwargs = dict(\n",
    "        model=str(model_dir),\n",
    "        dtype=dtype,\n",
    "        tensor_parallel_size=tensor_parallel_size,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "        disable_log_stats=True,   \n",
    "    )\n",
    "    if max_model_len is not None:\n",
    "        kwargs[\"max_model_len\"] = max_model_len\n",
    "    return LLM(**kwargs)\n",
    "\n",
    "\n",
    "def vllm_generate_solutions(\n",
    "    llm: LLM,\n",
    "    records: List[Dict[str, Any]],\n",
    "    batch_size: int = 32,\n",
    "    max_new_tokens: int = 512,\n",
    "    tqdm_desc: str = \"vLLM inference\",\n",
    ") -> List[str]:\n",
    "    sp = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    all_solutions: List[str] = []\n",
    "\n",
    "    # tqdm: batch-level progress\n",
    "    for i in tqdm(\n",
    "        range(0, len(records), batch_size),\n",
    "        desc=tqdm_desc,\n",
    "        total=(len(records) + batch_size - 1) // batch_size,\n",
    "    ):\n",
    "        batch = records[i:i+batch_size]\n",
    "\n",
    "        # vLLM chat: needs input like List[List[{\"role\",\"content\"}]]\n",
    "        messages_list = [r[\"prompt\"] for r in batch]\n",
    "\n",
    "        outs = llm.chat(messages_list, sampling_params=sp)\n",
    "        sols = [o.outputs[0].text for o in outs]\n",
    "        all_solutions.extend(sols)\n",
    "\n",
    "    return all_solutions\n",
    "\n",
    "\n",
    "def eval_records_with_vllm(\n",
    "    model_dir: str,\n",
    "    records: List[Dict[str, Any]],\n",
    "    batch_size: int = 32,\n",
    "    max_new_tokens: int = 512,\n",
    "    gpu_memory_utilization: float = 0.90,\n",
    "    dtype: str = \"auto\",\n",
    "    max_model_len: int | None = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    ") -> Tuple[float, List[bool], List[str]]:\n",
    "    llm = build_vllm_llm(\n",
    "        model_dir=model_dir,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "        dtype=dtype,\n",
    "        max_model_len=max_model_len,\n",
    "        tensor_parallel_size=tensor_parallel_size,\n",
    "    )\n",
    "\n",
    "    solutions = vllm_generate_solutions(\n",
    "        llm=llm,\n",
    "        records=records,\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        tqdm_desc=\"vLLM inference\",\n",
    "    )\n",
    "\n",
    "    oks: List[bool] = []\n",
    "    # tqdm: score-level progress\n",
    "    for r, sol in tqdm(\n",
    "        zip(records, solutions),\n",
    "        desc=\"compute_score\",\n",
    "        total=len(records),\n",
    "    ):\n",
    "        gt = r[\"reward_model\"][\"ground_truth\"]\n",
    "        ok = bool(compute_score(sol, gt))\n",
    "        oks.append(ok)\n",
    "\n",
    "    acc = sum(oks) / max(1, len(oks))\n",
    "    return acc, oks, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 17:37:18 [utils.py:233] non-default args: {'disable_log_stats': True, 'model': '/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60'}\n",
      "INFO 01-28 17:37:18 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 17:37:18 [model.py:1510] Using max model len 40960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 17:37:18 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-28 17:37:18 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/shared/michaelw/venvs/miniconda3/envs/verl/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 17:37:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:25 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:25 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60', speculative_config=None, tokenizer='/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m W0128 17:37:26.727000 1692977 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m W0128 17:37:26.727000 1692977 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:27 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:27 [gpu_model_runner.py:2602] Starting to load model /mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:27 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:27 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.97it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.96it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:28 [default_loader.py:267] Loading weights took 0.28 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:28 [gpu_model_runner.py:2653] Model loading took 1.1201 GiB and 0.536634 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:34 [backends.py:548] Using cache directory: /mnt/local/shared//michaelw/.cache/vllm/torch_compile_cache/158d2f012a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:34 [backends.py:559] Dynamo bytecode transform time: 5.55 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:37 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.479 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:38 [monitor.py:34] torch.compile takes 5.55 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:39 [gpu_worker.py:298] Available KV cache memory: 67.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:39 [kv_cache_utils.py:1087] GPU KV cache size: 635,776 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:39 [kv_cache_utils.py:1091] Maximum concurrency for 40,960 tokens per request: 15.52x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m All deep_gemm operations loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m 2026-01-28 17:37:39,771 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m 2026-01-28 17:37:39,802 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 30.34it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 67/67 [00:01<00:00, 41.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:44 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took -0.22 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m INFO 01-28 17:37:44 [core.py:210] init engine (profile, create kv cache, warmup model) took 15.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1692977)\u001b[0;0m The tokenizer you are loading from '/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 17:37:44 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0796dbc01a4e12b44c74ca5b0edf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vLLM inference:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/mnt/local/shared/michaelw/mlf2/verl/reproduce/grpo/checkpoints/mw_verl_recipe_reasoning/openthoughts-grpo-qwen3_0p6b_fsdp2/_merged_hf_cache/global_step_60' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 17:37:45 [chat_utils.py:560] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699368c453854eb595c336220af4b30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ee10fe96bb4ffebd46ec97bd7174dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3957c7337c004a519f43940189f044a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecb1577b38545e8870281eb8fecc012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631a58fa9c694dfba8639eb5d8776213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa86cb44ab7047938e1818db5dfa92f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525f174634f34d1b85e2422aa68d2e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e89d73f7a543419150aa795b076c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c6570edfa64987a46760f8d574783e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_score:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W128 17:37:52.419279483 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc@64: 0.140625\n",
      "example pred: <think>\n",
      "Okay, so I need to find the smallest possible value of the sum of absolute values: |x + 1| + |x + 3| + |x + 7|. Hmm, absolute value functions can sometimes be tricky because they change their behavior depending on the value of x. I remember that the sum of absolute values often has a minimum at a point where the expressions inside the absolute values change sign. But since there are three \n",
      "example gt: 6\n"
     ]
    }
   ],
   "source": [
    "acc, oks, sols = eval_records_with_vllm(\n",
    "    model_dir=str(model_dir),\n",
    "    records=records[:64],\n",
    "    batch_size=16,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(\"acc@64:\", acc)\n",
    "print(\"example pred:\", sols[0][:400])\n",
    "print(\"example gt:\", records[0][\"reward_model\"][\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
