# Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
_target_: verl.workers.config.DistillationConfig

# specify the default per-component configs
defaults:

  # distillation config, inheriting from trainer/config/ref/ref.yaml
  # Use absolute path + @_here_ to flatten fields at current level
  - /ref/ref@_here_

  # <folder_name>@<field_name>.<field_name>: <yaml_file_name>
  # Model config for teacher
  - teacher_model@teacher_models: models

  # load the default config, then apply the fields in the current yaml
  # self config override anything above
  - _self_

# Whether to enable distillation.
enabled: false

# Whether to enable separate resource pool for teacher model(s)
enable_resource_pool: false

# Number of GPUs per node to use for distillation teacher model(s)
n_gpus_per_node: 8

# Number of nodes to use for distillation teacher model(s)
nnodes: 0

# distillation loss config
distillation_loss:

  # # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.workers.config.DistillationLossConfig


  # Loss function mode for distillation
  loss_mode: k3

  # If distillation loss requires top-k logits, this is the value of k
  topk: 32

  # Whether the loss should just be the distillation loss or a combination of the distillation loss and the policy loss
  use_policy_loss: true

  # The coef of the distillation loss when use_policy_loss is true
  distillation_loss_coef: 1.0

  # Jensen-Shannon Divergence beta for jsd loss mode.
  jsd_beta: 0.5

  # Optional max clamp value for distillation loss. If null, no clamping is applied.
  loss_max_clamp: null

  # Optional min clamp value for log probabilities for stability, e.g., log q - log p where p or q are very close to zero.
  log_prob_min_clamp: null