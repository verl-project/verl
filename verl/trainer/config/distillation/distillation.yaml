# Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
_target_: verl.workers.config.DistillationConfig

# specify the default per-component configs
defaults:

  # <folder_name>@<field_name>.<field_name>: <yaml_file_name>

  # load the default config, then apply the fields in the current yaml
  # self config override anything above
  - _self_

# Whether to enable distillation.
enabled: false

# we launch num_workers teacher managers to parallelize the teacher logprob computation
num_workers: 8

# distillation loss config
distillation_loss:

  # # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.workers.config.DistillationLossConfig


  # Loss function mode for distillation
  loss_mode: k3

  # If distillation loss requires top-k logits, this is the value of k
  topk: 32

  # Whether to include task rewards alongside distillation loss.
  use_task_rewards: true


  # The coef of the distillation loss when use_task_rewards is true
  distillation_loss_coef: 1.0

  # Optional max clamp value for distillation loss. If null, no clamping is applied.
  loss_max_clamp: null

  # Optional min clamp value for log probabilities for stability, e.g., log q - log p where p or q are very close to zero.
  log_prob_min_clamp: null

  # Whether to incorporate distillation loss as a reward.
  use_policy_gradient: false

  # Name of the policy loss to use when use_policy_gradient is true.
  policy_loss_mode: "vanilla"

  # PPO clipping ratio for policy loss.
  clip_ratio: 0.2

  # Lower bound for PPO clipping ratio.
  clip_ratio_low: 0.2 

  # Upper bound for PPO clipping ratio.
  clip_ratio_high: 0.2


# teacher model config
teacher_model:
  _target_: verl.workers.config.DistillationTeacherModelConfig

  # Whether to enable separate resource pool for teacher model(s)
  enable_resource_pool: false

  # Number of GPUs per node to use for distillation teacher model(s)
  n_gpus_per_node: 8

  # Number of nodes to use for distillation teacher model(s)
  nnodes: 0

  model_path: null

  inference:
    _target_: verl.workers.config.RolloutConfig
    name: ${oc.select:actor_rollout_ref.rollout.name}
    dtype: ${oc.select:actor_rollout_ref.rollout.dtype}
    gpu_memory_utilization: 0.5
    enforce_eager: true
    cudagraph_capture_sizes: null
    free_cache_engine: true
    data_parallel_size: 1
    expert_parallel_size: 1
    tensor_model_parallel_size: 2
    max_num_batched_tokens: 8192
    max_model_len: null
    max_num_seqs: 1024
    load_format: auto
    engine_kwargs: {}
    limit_images: null
    enable_chunked_prefill: true
    enable_prefix_caching: true
    disable_log_stats: true
    skip_tokenizer_init: false

    # prompt_length: ${oc.select:actor_rollout_ref.rollout.prompt_length}
    # response_length: ${oc.select:actor_rollout_ref.rollout.response_length}
    prompt_length: 1
    response_length: 1

  