# Target class for this configuration
_target_: verl.workers.config.AutomodelEngineConfig

# Backend strategy identifier
strategy: automodel

# Distributed training strategy: "fsdp2", "megatron_fsdp", or "ddp"
distributed_strategy: fsdp2

# Parallelism sizes
tp_size: 1
pp_size: 1
cp_size: 1
ep_size: 1

# Whether to offload model parameters to CPU
param_offload: false

# Whether to offload optimizer state to CPU
optimizer_offload: false

# Whether to enable activation checkpointing
activation_checkpointing: false

# Whether to enable FP8 training
enable_fp8: false

# Whether to enable torch.compile for the model
enable_compile: false

# Model data type for loading weights ("fp32", "bf16", "fp16")
model_dtype: fp32

# Attention implementation ("sdpa", "flash_attention_2", "eager")
attn_implementation: sdpa

# Random seed for reproducibility
seed: 42

# Whether to enable full determinism for distributed training, only for debugging
full_determinism: false

# Whether to use forward only mode
forward_only: false

# Whether to use torch compile for entropy computation
use_torch_compile: false

# Whether to use chunked entropy computation
entropy_from_logits_with_chunking: false

# Whether to use checkpointing for entropy computation
entropy_checkpointing: false
