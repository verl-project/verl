{"step": 1, "data": {"grad_norm": 22.431764602661133, "train/loss": 1.3715933561325073, "train/lr": 5.0000000000000004e-08, "train/mfu": 0.016903332670116254, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 2, "data": {"grad_norm": 22.014636993408203, "train/loss": 1.4289090633392334, "train/lr": 1.0000000000000001e-07, "train/mfu": 0.023606615658061672, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 3, "data": {"grad_norm": 21.283287048339844, "train/loss": 1.3440947532653809, "train/lr": 1.5000000000000002e-07, "train/mfu": 0.024049366499814834, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 4, "data": {"grad_norm": 21.410837173461914, "train/loss": 1.3846491575241089, "train/lr": 2.0000000000000002e-07, "train/mfu": 0.02353589092005477, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 5, "data": {"grad_norm": 22.732004165649414, "train/loss": 1.4219731092453003, "train/lr": 2.5000000000000004e-07, "train/mfu": 0.0236846376671156, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 6, "data": {"grad_norm": 20.608850479125977, "train/loss": 1.369738221168518, "train/lr": 3.0000000000000004e-07, "train/mfu": 0.024035795379303043, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 7, "data": {"grad_norm": 22.250852584838867, "train/loss": 1.3786966800689697, "train/lr": 3.5000000000000004e-07, "train/mfu": 0.024060288478039583, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 8, "data": {"grad_norm": 22.574573516845703, "train/loss": 1.3694555759429932, "train/lr": 4.0000000000000003e-07, "train/mfu": 0.024081136274721134, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 9, "data": {"grad_norm": 22.064455032348633, "train/loss": 1.335906744003296, "train/lr": 4.5000000000000003e-07, "train/mfu": 0.02379345896358051, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 10, "data": {"grad_norm": 23.05801773071289, "train/loss": 1.4398689270019531, "train/lr": 5.000000000000001e-07, "train/mfu": 0.023645500762946376, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 11, "data": {"grad_norm": 20.837890625, "train/loss": 1.393887996673584, "train/lr": 5.5e-07, "train/mfu": 0.02385966002768259, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 12, "data": {"grad_norm": 21.26848793029785, "train/loss": 1.380075454711914, "train/lr": 6.000000000000001e-07, "train/mfu": 0.02328854790770388, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 13, "data": {"grad_norm": 20.69782829284668, "train/loss": 1.3615983724594116, "train/lr": 6.5e-07, "train/mfu": 0.023305931707423944, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 14, "data": {"grad_norm": 19.220712661743164, "train/loss": 1.3533103466033936, "train/lr": 7.000000000000001e-07, "train/mfu": 0.02315897346660224, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 15, "data": {"grad_norm": 19.367874145507812, "train/loss": 1.3279600143432617, "train/lr": 7.5e-07, "train/mfu": 0.023199283056278858, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 16, "data": {"grad_norm": 17.203998565673828, "train/loss": 1.2710164785385132, "train/lr": 8.000000000000001e-07, "train/mfu": 0.023243985634386777, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 17, "data": {"grad_norm": 18.238985061645508, "train/loss": 1.3436753749847412, "train/lr": 8.500000000000001e-07, "train/mfu": 0.023663139983529848, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 18, "data": {"grad_norm": 17.19182014465332, "train/loss": 1.285698413848877, "train/lr": 9.000000000000001e-07, "train/mfu": 0.023216558543289564, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 19, "data": {"grad_norm": 18.859025955200195, "train/loss": 1.2885916233062744, "train/lr": 9.500000000000001e-07, "train/mfu": 0.023236653661537217, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 20, "data": {"grad_norm": 16.18413543701172, "train/loss": 1.1801354885101318, "train/lr": 1.0000000000000002e-06, "train/mfu": 0.023734303070387704, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 21, "data": {"grad_norm": 16.267929077148438, "train/loss": 1.1885178089141846, "train/lr": 1.0500000000000001e-06, "train/mfu": 0.023497211909871786, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 22, "data": {"grad_norm": 15.634798049926758, "train/loss": 1.1540162563323975, "train/lr": 1.1e-06, "train/mfu": 0.02323374269170256, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 23, "data": {"grad_norm": 15.996915817260742, "train/loss": 1.1258094310760498, "train/lr": 1.1500000000000002e-06, "train/mfu": 0.02324165752595956, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 24, "data": {"grad_norm": 16.203397750854492, "train/loss": 1.1265039443969727, "train/lr": 1.2000000000000002e-06, "train/mfu": 0.02334256248830461, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 25, "data": {"grad_norm": 18.784067153930664, "train/loss": 1.1668775081634521, "train/lr": 1.25e-06, "train/mfu": 0.02305621249952187, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 26, "data": {"grad_norm": 17.5768985748291, "train/loss": 1.0621650218963623, "train/lr": 1.3e-06, "train/mfu": 0.02338313038481113, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 27, "data": {"grad_norm": 10.826333999633789, "train/loss": 0.9031972885131836, "train/lr": 1.3500000000000002e-06, "train/mfu": 0.023553697005516606, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 28, "data": {"grad_norm": 10.154480934143066, "train/loss": 0.8750895857810974, "train/lr": 1.4000000000000001e-06, "train/mfu": 0.023308714746174818, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 29, "data": {"grad_norm": 9.902365684509277, "train/loss": 0.8679297566413879, "train/lr": 1.45e-06, "train/mfu": 0.023160372861948424, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 30, "data": {"grad_norm": 10.13793659210205, "train/loss": 0.8454810976982117, "train/lr": 1.5e-06, "train/mfu": 0.023417601372124563, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 31, "data": {"grad_norm": 9.32671070098877, "train/loss": 0.796523928642273, "train/lr": 1.5500000000000002e-06, "train/mfu": 0.02369724594981963, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 32, "data": {"grad_norm": 8.943648338317871, "train/loss": 0.8308395743370056, "train/lr": 1.6000000000000001e-06, "train/mfu": 0.0232335742165626, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 33, "data": {"grad_norm": 8.784411430358887, "train/loss": 0.8441179990768433, "train/lr": 1.6500000000000003e-06, "train/mfu": 0.023169892117154242, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 34, "data": {"grad_norm": 8.502776145935059, "train/loss": 0.7489649653434753, "train/lr": 1.7000000000000002e-06, "train/mfu": 0.02302042979303834, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 35, "data": {"grad_norm": 7.488917350769043, "train/loss": 0.7245672941207886, "train/lr": 1.75e-06, "train/mfu": 0.023519656953394395, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 36, "data": {"grad_norm": 6.278234958648682, "train/loss": 0.7257496118545532, "train/lr": 1.8000000000000001e-06, "train/mfu": 0.023636566584493878, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 37, "data": {"grad_norm": 5.725965976715088, "train/loss": 0.6612715125083923, "train/lr": 1.85e-06, "train/mfu": 0.023122437243560116, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 38, "data": {"grad_norm": 5.805703639984131, "train/loss": 0.6682080030441284, "train/lr": 1.9000000000000002e-06, "train/mfu": 0.023207053586874922, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 39, "data": {"grad_norm": 6.166415691375732, "train/loss": 0.641569197177887, "train/lr": 1.9500000000000004e-06, "train/mfu": 0.023110919563402218, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 40, "data": {"grad_norm": 5.32749605178833, "train/loss": 0.6112098693847656, "train/lr": 2.0000000000000003e-06, "train/mfu": 0.023030484626921702, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 41, "data": {"grad_norm": 4.7024407386779785, "train/loss": 0.5902038812637329, "train/lr": 2.05e-06, "train/mfu": 0.023262806544055905, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 42, "data": {"grad_norm": 3.541473865509033, "train/loss": 0.5547860860824585, "train/lr": 2.1000000000000002e-06, "train/mfu": 0.023185177563644122, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 43, "data": {"grad_norm": 3.0068347454071045, "train/loss": 0.5738230347633362, "train/lr": 2.15e-06, "train/mfu": 0.02357133402979474, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 44, "data": {"grad_norm": 2.9743735790252686, "train/loss": 0.6452996730804443, "train/lr": 2.2e-06, "train/mfu": 0.023537441156083406, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 45, "data": {"grad_norm": 2.843947649002075, "train/loss": 0.5950402021408081, "train/lr": 2.25e-06, "train/mfu": 0.02341697279545206, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 46, "data": {"grad_norm": 2.583003044128418, "train/loss": 0.568558394908905, "train/lr": 2.3000000000000004e-06, "train/mfu": 0.0227374652762699, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 47, "data": {"grad_norm": 2.295077085494995, "train/loss": 0.5195156335830688, "train/lr": 2.35e-06, "train/mfu": 0.023232208649091066, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 48, "data": {"grad_norm": 2.3333325386047363, "train/loss": 0.5637872815132141, "train/lr": 2.4000000000000003e-06, "train/mfu": 0.02327962217037756, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 49, "data": {"grad_norm": 2.4140336513519287, "train/loss": 0.5115342140197754, "train/lr": 2.4500000000000003e-06, "train/mfu": 0.02316802561047289, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 50, "data": {"grad_norm": 2.27443528175354, "train/loss": 0.5994061231613159, "train/lr": 2.5e-06, "train/mfu": 0.023388759759298817, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 51, "data": {"grad_norm": 2.109952926635742, "train/loss": 0.5332620739936829, "train/lr": 2.55e-06, "train/mfu": 0.0234590097797973, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 52, "data": {"grad_norm": 2.388593912124634, "train/loss": 0.5174912214279175, "train/lr": 2.6e-06, "train/mfu": 0.023188457594593655, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 53, "data": {"grad_norm": 2.129444122314453, "train/loss": 0.5455602407455444, "train/lr": 2.6500000000000005e-06, "train/mfu": 0.023319271133275254, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 54, "data": {"grad_norm": 2.2141425609588623, "train/loss": 0.5348550081253052, "train/lr": 2.7000000000000004e-06, "train/mfu": 0.023497613554754767, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 55, "data": {"grad_norm": 2.099236011505127, "train/loss": 0.5272574424743652, "train/lr": 2.7500000000000004e-06, "train/mfu": 0.02362149429345494, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 56, "data": {"grad_norm": 2.072228193283081, "train/loss": 0.5215773582458496, "train/lr": 2.8000000000000003e-06, "train/mfu": 0.02314663654141362, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 57, "data": {"grad_norm": 2.017437696456909, "train/loss": 0.4831765294075012, "train/lr": 2.85e-06, "train/mfu": 0.023243150233147315, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"grad_norm": 1.799726128578186, "train/loss": 0.5307933688163757, "train/lr": 2.9e-06, "train/mfu": 0.023622952380941677, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 58, "data": {"val/loss": 0.5351342558860779}}
{"step": 59, "data": {"grad_norm": 1.9188817739486694, "train/loss": 0.5069695115089417, "train/lr": 2.95e-06, "train/mfu": 0.023130289398089664, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 60, "data": {"grad_norm": 1.7184488773345947, "train/loss": 0.4983491897583008, "train/lr": 3e-06, "train/mfu": 0.02312240115191519, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 61, "data": {"grad_norm": 1.6530362367630005, "train/loss": 0.49543124437332153, "train/lr": 3.05e-06, "train/mfu": 0.023861047272318386, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 62, "data": {"grad_norm": 1.7791953086853027, "train/loss": 0.4818711280822754, "train/lr": 3.1000000000000004e-06, "train/mfu": 0.02375692695682144, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 63, "data": {"grad_norm": 1.693686842918396, "train/loss": 0.5122075080871582, "train/lr": 3.1500000000000003e-06, "train/mfu": 0.02306614754528229, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 64, "data": {"grad_norm": 1.6382768154144287, "train/loss": 0.48986899852752686, "train/lr": 3.2000000000000003e-06, "train/mfu": 0.02323500365215701, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 65, "data": {"grad_norm": 1.7663357257843018, "train/loss": 0.46033018827438354, "train/lr": 3.2500000000000002e-06, "train/mfu": 0.02372439604878323, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 66, "data": {"grad_norm": 1.530410647392273, "train/loss": 0.4768458902835846, "train/lr": 3.3000000000000006e-06, "train/mfu": 0.023249088030004333, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 67, "data": {"grad_norm": 1.5422922372817993, "train/loss": 0.463981568813324, "train/lr": 3.3500000000000005e-06, "train/mfu": 0.023040909017654574, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 68, "data": {"grad_norm": 1.8292659521102905, "train/loss": 0.4966815710067749, "train/lr": 3.4000000000000005e-06, "train/mfu": 0.023454577996296345, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 69, "data": {"grad_norm": 1.5401643514633179, "train/loss": 0.4887933135032654, "train/lr": 3.45e-06, "train/mfu": 0.023485183970143018, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 70, "data": {"grad_norm": 1.5286914110183716, "train/loss": 0.45061051845550537, "train/lr": 3.5e-06, "train/mfu": 0.02327659897043192, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 71, "data": {"grad_norm": 1.6037951707839966, "train/loss": 0.47118231654167175, "train/lr": 3.5500000000000003e-06, "train/mfu": 0.02344267944792007, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 72, "data": {"grad_norm": 1.644451379776001, "train/loss": 0.48125216364860535, "train/lr": 3.6000000000000003e-06, "train/mfu": 0.02328704480647956, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 73, "data": {"grad_norm": 1.6562443971633911, "train/loss": 0.46241092681884766, "train/lr": 3.65e-06, "train/mfu": 0.023071609860041226, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 74, "data": {"grad_norm": 1.5759953260421753, "train/loss": 0.4554379880428314, "train/lr": 3.7e-06, "train/mfu": 0.023570754353258293, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 75, "data": {"grad_norm": 1.590136170387268, "train/loss": 0.46370336413383484, "train/lr": 3.7500000000000005e-06, "train/mfu": 0.023648297856300213, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 76, "data": {"grad_norm": 1.7061762809753418, "train/loss": 0.499217689037323, "train/lr": 3.8000000000000005e-06, "train/mfu": 0.023379090116865127, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 77, "data": {"grad_norm": 1.4481098651885986, "train/loss": 0.4309428334236145, "train/lr": 3.85e-06, "train/mfu": 0.023428870493587793, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 78, "data": {"grad_norm": 1.4899710416793823, "train/loss": 0.44713497161865234, "train/lr": 3.900000000000001e-06, "train/mfu": 0.02361700666819452, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 79, "data": {"grad_norm": 1.5905907154083252, "train/loss": 0.47638189792633057, "train/lr": 3.95e-06, "train/mfu": 0.023743348943468766, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 80, "data": {"grad_norm": 1.4953445196151733, "train/loss": 0.47143658995628357, "train/lr": 4.000000000000001e-06, "train/mfu": 0.023496546787898403, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 81, "data": {"grad_norm": 1.5411522388458252, "train/loss": 0.4957761764526367, "train/lr": 4.05e-06, "train/mfu": 0.023752780427183245, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 82, "data": {"grad_norm": 1.5530781745910645, "train/loss": 0.4687430262565613, "train/lr": 4.1e-06, "train/mfu": 0.023141426988823783, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 83, "data": {"grad_norm": 1.4764772653579712, "train/loss": 0.44745492935180664, "train/lr": 4.15e-06, "train/mfu": 0.023524321358565827, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 84, "data": {"grad_norm": 1.4507882595062256, "train/loss": 0.45137274265289307, "train/lr": 4.2000000000000004e-06, "train/mfu": 0.023752734832978, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 85, "data": {"grad_norm": 1.80875563621521, "train/loss": 0.49049317836761475, "train/lr": 4.25e-06, "train/mfu": 0.023303635974397163, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 86, "data": {"grad_norm": 1.554384469985962, "train/loss": 0.46321865916252136, "train/lr": 4.3e-06, "train/mfu": 0.023479355958450483, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 87, "data": {"grad_norm": 1.5597177743911743, "train/loss": 0.4498666822910309, "train/lr": 4.350000000000001e-06, "train/mfu": 0.023535180891378536, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 88, "data": {"grad_norm": 1.5809719562530518, "train/loss": 0.46756476163864136, "train/lr": 4.4e-06, "train/mfu": 0.023799002282746465, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 89, "data": {"grad_norm": 1.585106372833252, "train/loss": 0.47991079092025757, "train/lr": 4.450000000000001e-06, "train/mfu": 0.023440115889298053, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 90, "data": {"grad_norm": 1.686423897743225, "train/loss": 0.47174420952796936, "train/lr": 4.5e-06, "train/mfu": 0.02340208523404382, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 91, "data": {"grad_norm": 1.526764988899231, "train/loss": 0.4419078230857849, "train/lr": 4.5500000000000005e-06, "train/mfu": 0.023655820940783907, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 92, "data": {"grad_norm": 1.607263445854187, "train/loss": 0.4692344069480896, "train/lr": 4.600000000000001e-06, "train/mfu": 0.02358055631501643, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 93, "data": {"grad_norm": 1.4867252111434937, "train/loss": 0.4724048972129822, "train/lr": 4.65e-06, "train/mfu": 0.0229919159141585, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 94, "data": {"grad_norm": 1.4581780433654785, "train/loss": 0.45690399408340454, "train/lr": 4.7e-06, "train/mfu": 0.023264174543987708, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 95, "data": {"grad_norm": 1.5585882663726807, "train/loss": 0.4712291955947876, "train/lr": 4.75e-06, "train/mfu": 0.023630601125846216, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 96, "data": {"grad_norm": 1.4406999349594116, "train/loss": 0.4720529615879059, "train/lr": 4.800000000000001e-06, "train/mfu": 0.023272141483083136, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 97, "data": {"grad_norm": 1.5345925092697144, "train/loss": 0.4732741713523865, "train/lr": 4.85e-06, "train/mfu": 0.02317801507798433, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 98, "data": {"grad_norm": 1.4769235849380493, "train/loss": 0.4238469898700714, "train/lr": 4.9000000000000005e-06, "train/mfu": 0.023316456268033543, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 99, "data": {"grad_norm": 1.5023456811904907, "train/loss": 0.5002506375312805, "train/lr": 4.95e-06, "train/mfu": 0.023487795573847486, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 100, "data": {"grad_norm": 1.4692397117614746, "train/loss": 0.4506361782550812, "train/lr": 5e-06, "train/mfu": 0.023297422725845715, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 101, "data": {"grad_norm": 1.4389604330062866, "train/loss": 0.42525196075439453, "train/lr": 5.050000000000001e-06, "train/mfu": 0.02338580748206994, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 102, "data": {"grad_norm": 1.701578974723816, "train/loss": 0.5042070150375366, "train/lr": 5.1e-06, "train/mfu": 0.023611839817040653, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 103, "data": {"grad_norm": 1.5178085565567017, "train/loss": 0.4492676556110382, "train/lr": 5.150000000000001e-06, "train/mfu": 0.023256547935976438, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 104, "data": {"grad_norm": 1.437851071357727, "train/loss": 0.4528452455997467, "train/lr": 5.2e-06, "train/mfu": 0.02367301048991679, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 105, "data": {"grad_norm": 1.3820306062698364, "train/loss": 0.4304048717021942, "train/lr": 5.2500000000000006e-06, "train/mfu": 0.02335401029382508, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 106, "data": {"grad_norm": 1.4566198587417603, "train/loss": 0.44001591205596924, "train/lr": 5.300000000000001e-06, "train/mfu": 0.02339973689237522, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 107, "data": {"grad_norm": 1.4865374565124512, "train/loss": 0.448560893535614, "train/lr": 5.3500000000000004e-06, "train/mfu": 0.02314688765370747, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 108, "data": {"grad_norm": 1.4780592918395996, "train/loss": 0.4638853073120117, "train/lr": 5.400000000000001e-06, "train/mfu": 0.02339014885571207, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 109, "data": {"grad_norm": 1.4211686849594116, "train/loss": 0.43059468269348145, "train/lr": 5.450000000000001e-06, "train/mfu": 0.023439663658823412, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 110, "data": {"grad_norm": 1.4903554916381836, "train/loss": 0.4567106068134308, "train/lr": 5.500000000000001e-06, "train/mfu": 0.023088158346192253, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 111, "data": {"grad_norm": 1.4818557500839233, "train/loss": 0.43789902329444885, "train/lr": 5.550000000000001e-06, "train/mfu": 0.023374290902973056, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 112, "data": {"grad_norm": 1.4771373271942139, "train/loss": 0.44244876503944397, "train/lr": 5.600000000000001e-06, "train/mfu": 0.023587235964472347, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 113, "data": {"grad_norm": 1.518905520439148, "train/loss": 0.4164450168609619, "train/lr": 5.65e-06, "train/mfu": 0.023105492558296605, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 114, "data": {"grad_norm": 1.4814915657043457, "train/loss": 0.47595471143722534, "train/lr": 5.7e-06, "train/mfu": 0.023256401912629284, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 115, "data": {"grad_norm": 1.524381160736084, "train/loss": 0.47495773434638977, "train/lr": 5.75e-06, "train/mfu": 0.023793204571487026, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 116, "data": {"grad_norm": 1.476032018661499, "train/loss": 0.484752893447876, "train/lr": 5.8e-06, "train/mfu": 0.02326643557327155, "train/global_tokens": 0.0, "train/total_tokens(B)": 0.0}}
{"step": 116, "data": {"val/loss": 0.46795544028282166}}
